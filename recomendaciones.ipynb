{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdc7fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Notebook del Motor de Recomendación ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 3: MOTOR DE RECOMENDACIÓN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Importación de Librerías ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # Para cargar el modelo y otros artefactos\n",
    "import random  # Para seleccionar un cliente de ejemplo\n",
    "\n",
    "print(\"--- Iniciando Notebook del Motor de Recomendación ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b4eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando artefactos necesarios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefactos y datos cargados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Carga de Artefactos Necesarios ---\n",
    "print(\"Cargando artefactos necesarios...\")\n",
    "try:\n",
    "    modelo = joblib.load('modelo_recomendacion_final.pkl')\n",
    "    training_columns = joblib.load('training_columns.pkl')\n",
    "    \n",
    "    # Cargar los datos originales\n",
    "    df_articles = pd.read_csv('Datos/articles.csv')\n",
    "    df_customers = pd.read_csv('Datos/customers.csv')\n",
    "    df_trans = pd.read_csv('Datos/young_female_trans.csv')\n",
    "    \n",
    "    # --- ¡LA LÍNEA DE CORRECCIÓN CLAVE! ---\n",
    "    # Estandarizamos el tipo de 'article_id' en df_articles a string (object)\n",
    "    # para asegurar que todos los merges futuros funcionen correctamente.\n",
    "    df_articles['article_id'] = df_articles['article_id'].astype(str)\n",
    "    # ------------------------------------\n",
    "\n",
    "    print(\"Artefactos y datos cargados exitosamente.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: No se encontró el archivo {e.filename}.\")\n",
    "    print(\"Asegúrate de que los archivos están en el directorio.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec6e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a97ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando perfiles de usuario y características de artículos...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns for key 'article_id'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Combinar dataframes (versión simplificada del notebook 1)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_trans, df_customers, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m full_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1508\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1502\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m   1504\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1505\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1506\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1507\u001b[0m     ):\n\u001b[1;32m-> 1508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns for key 'article_id'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# --- 3. Preparación de Datos Auxiliares ---\n",
    "# Re-creamos los perfiles de usuario y características de artículos.\n",
    "# Esto es necesario para construir los vectores de predicción.\n",
    "print(\"\\nPreparando perfiles de usuario y características de artículos...\")\n",
    "\n",
    "# Combinar dataframes\n",
    "merged_df = pd.merge(df_trans, df_customers, on='customer_id', how='left')\n",
    "\n",
    "# Asegurar tipos ANTES del segundo merge\n",
    "merged_df['article_id'] = merged_df['article_id'].astype(str)\n",
    "df_articles['article_id'] = df_articles['article_id'].astype(str) # Asegúrate de que esto también se hizo en la celda de carga\n",
    "full_df = pd.merge(merged_df, df_articles, on='article_id', how='left')\n",
    "\n",
    "full_df['age'].fillna(full_df['age'].median(), inplace=True)\n",
    "full_df['customer_id'] = full_df['customer_id'].astype(str)\n",
    "# 'article_id' ya es string aquí, así que la siguiente línea es redundante pero no hace daño\n",
    "full_df['article_id'] = full_df['article_id'].astype(str)\n",
    "\n",
    "# Crear perfiles de usuario\n",
    "user_profiles = full_df.groupby('customer_id').agg(\n",
    "    age=('age', 'first'),\n",
    "    avg_price_paid=('price', 'mean'),\n",
    "    total_articles_bought=('article_id', 'count'),\n",
    "    fav_product_type=('product_type_name', lambda x: x.mode()[0] if not x.empty else 'Unknown'),\n",
    "    fav_color=('colour_group_name', lambda x: x.mode()[0] if not x.empty else 'Unknown'),\n",
    "    fav_department=('department_name', lambda x: x.mode()[0] if not x.empty else 'Unknown')\n",
    ").reset_index()\n",
    "\n",
    "# --- BLOQUE CORREGIDO PARA CREAR final_item_features ---\n",
    "\n",
    "# Crear popularidad del artículo\n",
    "item_popularity = full_df.groupby('article_id').agg(times_purchased=('customer_id', 'count')).reset_index()\n",
    "\n",
    "# Crear características estáticas (esta parte estaba bien)\n",
    "static_features = df_articles[['article_id', 'product_type_name', 'product_group_name', 'graphical_appearance_name', \n",
    "                                'colour_group_name', 'department_name', 'index_name', 'section_name', 'garment_group_name']]\n",
    "\n",
    "# --- ¡LA CORRECCIÓN ESTÁ AQUÍ! ---\n",
    "# Forzamos que la columna clave 'article_id' sea del mismo tipo (string) en ambos DFs antes de unirlos.\n",
    "static_features['article_id'] = static_features['article_id'].astype(str)\n",
    "item_popularity['article_id'] = item_popularity['article_id'].astype(str)\n",
    "# --------------------------------\n",
    "\n",
    "# Ahora el merge funcionará\n",
    "final_item_features = pd.merge(static_features, item_popularity, on='article_id', how='left').fillna(0)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Conjunto de todos los artículos y diccionario de compras por usuario\n",
    "# (Asegúrate que el tipo es consistente aquí también)\n",
    "all_articles = set(df_articles['article_id'].astype(str))\n",
    "df_trans['article_id'] = df_trans['article_id'].astype(str) # Añadir por seguridad\n",
    "purchased_items_dict = df_trans.groupby('customer_id')['article_id'].apply(set)\n",
    "\n",
    "del merged_df, full_df # Liberar memoria\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Perfiles y características listos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac067c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Función Principal de Recomendación ---\n",
    "# Esta función encapsula toda la lógica para generar recomendaciones para un solo cliente.\n",
    "def generar_recomendaciones(customer_id, modelo, n_recommendations=10):\n",
    "    \"\"\"\n",
    "    Genera una lista de N recomendaciones personalizadas para un cliente específico.\n",
    "    \"\"\"\n",
    "    print(f\"\\n==========================================================\")\n",
    "    print(f\"INICIANDO GENERACIÓN DE RECOMENDACIONES PARA EL CLIENTE: {customer_id}\")\n",
    "    print(f\"==========================================================\")\n",
    "    \n",
    "    # 1. Obtener el perfil del usuario a partir del ID\n",
    "    user_profile = user_profiles[user_profiles['customer_id'] == customer_id]\n",
    "    if user_profile.empty:\n",
    "        return f\"Error: Cliente con ID '{customer_id}' no encontrado.\"\n",
    "        \n",
    "    # 2. Obtener la lista de artículos que el cliente NO ha comprado (Candidatos)\n",
    "    items_comprados_por_usuario = purchased_items_dict.get(customer_id, set())\n",
    "    articulos_candidatos = list(all_articles - items_comprados_por_usuario)\n",
    "    \n",
    "    # Para eficiencia, si hay demasiados candidatos, tomamos una muestra aleatoria.\n",
    "    # Esto simula recomendar a partir de un subconjunto del catálogo (ej. novedades, populares).\n",
    "    if len(articulos_candidatos) > 5000:\n",
    "        articulos_candidatos = random.sample(articulos_candidatos, 5000)\n",
    "    print(f\"Paso 1: Se evaluarán {len(articulos_candidatos)} artículos candidatos.\")\n",
    "        \n",
    "    # 3. Crear el DataFrame para la predicción\n",
    "    df_predict = pd.DataFrame({'article_id': articulos_candidatos})\n",
    "    df_predict['customer_id'] = customer_id\n",
    "    \n",
    "    # 4. Enriquecer el DataFrame con las características del usuario y de los productos\n",
    "    df_predict = pd.merge(df_predict, user_profile, on='customer_id', how='left')\n",
    "    df_predict = pd.merge(df_predict, final_item_features, on='article_id', how='left')\n",
    "    \n",
    "    # 5. Preparar los datos para que coincidan con el formato de entrenamiento\n",
    "    print(\"Paso 2: Preparando características para el modelo...\")\n",
    "    # Seleccionar las mismas columnas de características que usamos para entrenar\n",
    "    X_pred = df_predict.drop(columns=['customer_id', 'article_id'])\n",
    "    \n",
    "    # Aplicar la misma codificación ordinal que en el entrenamiento\n",
    "    # Es crucial usar un encoder 'fittado' o re-codificar de la misma manera\n",
    "    categorical_features = X_pred.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    # Usamos el mismo OrdinalEncoder que antes, o creamos uno nuevo (para este ejemplo es suficiente)\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    # Importante: aquí hacemos fit_transform sobre las características de los candidatos\n",
    "    X_pred[categorical_features] = encoder.fit_transform(X_pred[categorical_features])\n",
    "\n",
    "    # 6. Alinear las columnas (Paso CRÍTICO para evitar errores)\n",
    "    # Nos aseguramos de que el DataFrame de predicción tenga exactamente las mismas columnas\n",
    "    # y en el mismo orden que los datos con los que se entrenó el modelo.\n",
    "    X_pred_aligned = X_pred.reindex(columns=training_columns, fill_value=0)\n",
    "\n",
    "    # 7. Realizar la predicción de probabilidades\n",
    "    print(\"Paso 3: Calculando puntuaciones de recomendación...\")\n",
    "    scores = modelo.predict_proba(X_pred_aligned)[:, 1]\n",
    "    \n",
    "    # 8. Unir puntuaciones y devolver el Top-N\n",
    "    df_predict['score'] = scores\n",
    "    recomendaciones = df_predict.sort_values('score', ascending=False).head(n_recommendations)\n",
    "    \n",
    "    # Añadir información legible (nombre del producto) para la salida final\n",
    "    recomendaciones_final = pd.merge(recomendaciones, df_articles[['article_id', 'prod_name']], \n",
    "                                     on='article_id', how='left')\n",
    "    \n",
    "    print(\"Paso 4: ¡Recomendaciones generadas!\")\n",
    "    return recomendaciones_final[['article_id', 'prod_name', 'score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EJEMPLO DE USO ---\n",
    "# Seleccionar un cliente aleatorio de nuestro diccionario de compras para asegurar que tiene historial\n",
    "lista_de_clientes = list(purchased_items_dict.keys())\n",
    "cliente_ejemplo = random.choice(lista_de_clientes)\n",
    "\n",
    "# Generar las recomendaciones para este cliente\n",
    "recomendaciones_para_cliente = generar_recomendaciones(cliente_ejemplo, modelo)\n",
    "\n",
    "# Mostrar los resultados de una manera bonita\n",
    "print(f\"\\n\\n✓✓✓ Top 10 Recomendaciones para el Cliente: {cliente_ejemplo} ✓✓✓\\n\")\n",
    "print(recomendaciones_para_cliente.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
