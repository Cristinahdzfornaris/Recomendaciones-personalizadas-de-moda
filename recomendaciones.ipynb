{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdc7fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Notebook del Motor de Recomendación ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 3: MOTOR DE RECOMENDACIÓN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Importación de Librerías ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # Para cargar el modelo y otros artefactos\n",
    "import random  # Para seleccionar un cliente de ejemplo\n",
    "\n",
    "print(\"--- Iniciando Notebook del Motor de Recomendación ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b4eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando artefactos necesarios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crist\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefactos y datos cargados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Carga de Artefactos Necesarios ---\n",
    "# Cargamos todo lo que preparamos en los notebooks anteriores.\n",
    "print(\"Cargando artefactos necesarios...\")\n",
    "try:\n",
    "    # Cargar el modelo final (el ajustado)\n",
    "    modelo = joblib.load('modelo_recomendacion_final.pkl')\n",
    "    # Cargar la lista de columnas con la que se entrenó el modelo\n",
    "    training_columns = joblib.load('training_columns.pkl')\n",
    "    # Cargar los datos originales para obtener información de productos y clientes\n",
    "    df_articles = pd.read_csv('Datos/articles.csv')\n",
    "    df_customers = pd.read_csv('Datos/customers.csv')\n",
    "    df_trans = pd.read_csv('Datos/young_female_trans.csv')\n",
    "    print(\"Artefactos y datos cargados exitosamente.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: No se encontró el archivo {e.filename}.\")\n",
    "    print(\"Asegúrate de que los archivos de datos y los artefactos guardados (.pkl) están en el directorio.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41a97ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparando perfiles de usuario y características de artículos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Local\\Temp\\ipykernel_996\\564352538.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  full_df['age'].fillna(full_df['age'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfiles y características listos.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Preparación de Datos Auxiliares ---\n",
    "# Re-creamos los perfiles de usuario y características de artículos.\n",
    "# Esto es necesario para construir los vectores de predicción.\n",
    "print(\"\\nPreparando perfiles de usuario y características de artículos...\")\n",
    "\n",
    "# Combinar dataframes (versión simplificada del notebook 1)\n",
    "merged_df = pd.merge(df_trans, df_customers, on='customer_id', how='left')\n",
    "full_df = pd.merge(merged_df, df_articles, on='article_id', how='left')\n",
    "full_df['age'].fillna(full_df['age'].median(), inplace=True)\n",
    "full_df['customer_id'] = full_df['customer_id'].astype(str)\n",
    "full_df['article_id'] = full_df['article_id'].astype(str)\n",
    "\n",
    "# Crear perfiles de usuario\n",
    "user_profiles = full_df.groupby('customer_id').agg(\n",
    "    age=('age', 'first'),\n",
    "    avg_price_paid=('price', 'mean'),\n",
    "    total_articles_bought=('article_id', 'count'),\n",
    "    fav_product_type=('product_type_name', lambda x: x.mode()[0] if not x.empty else 'Unknown'),\n",
    "    fav_color=('colour_group_name', lambda x: x.mode()[0] if not x.empty else 'Unknown'),\n",
    "    fav_department=('department_name', lambda x: x.mode()[0] if not x.empty else 'Unknown')\n",
    ").reset_index()\n",
    "\n",
    "# Crear características de artículos\n",
    "item_popularity = full_df.groupby('article_id').agg(times_purchased=('customer_id', 'count')).reset_index()\n",
    "# Seleccionamos las mismas columnas que en el notebook 2 para consistencia\n",
    "static_features = df_articles[['article_id', 'product_type_name', 'product_group_name', 'graphical_appearance_name', \n",
    "                                'colour_group_name', 'department_name', 'index_name', 'section_name', 'garment_group_name']].astype({'article_id': str})\n",
    "final_item_features = pd.merge(static_features, item_popularity, on='article_id', how='left').fillna(0)\n",
    "\n",
    "# Conjunto de todos los artículos y diccionario de compras por usuario\n",
    "all_articles = set(df_articles['article_id'].astype(str))\n",
    "purchased_items_dict = df_trans.groupby('customer_id')['article_id'].apply(set)\n",
    "\n",
    "del merged_df, full_df # Liberar memoria\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Perfiles y características listos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac067c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Función Principal de Recomendación ---\n",
    "# Esta función encapsula toda la lógica para generar recomendaciones para un solo cliente.\n",
    "def generar_recomendaciones(customer_id, modelo, n_recommendations=10):\n",
    "    \"\"\"\n",
    "    Genera una lista de N recomendaciones personalizadas para un cliente específico.\n",
    "    \"\"\"\n",
    "    print(f\"\\n==========================================================\")\n",
    "    print(f\"INICIANDO GENERACIÓN DE RECOMENDACIONES PARA EL CLIENTE: {customer_id}\")\n",
    "    print(f\"==========================================================\")\n",
    "    \n",
    "    # 1. Obtener el perfil del usuario a partir del ID\n",
    "    user_profile = user_profiles[user_profiles['customer_id'] == customer_id]\n",
    "    if user_profile.empty:\n",
    "        return f\"Error: Cliente con ID '{customer_id}' no encontrado.\"\n",
    "        \n",
    "    # 2. Obtener la lista de artículos que el cliente NO ha comprado (Candidatos)\n",
    "    items_comprados_por_usuario = purchased_items_dict.get(customer_id, set())\n",
    "    articulos_candidatos = list(all_articles - items_comprados_por_usuario)\n",
    "    \n",
    "    # Para eficiencia, si hay demasiados candidatos, tomamos una muestra aleatoria.\n",
    "    # Esto simula recomendar a partir de un subconjunto del catálogo (ej. novedades, populares).\n",
    "    if len(articulos_candidatos) > 5000:\n",
    "        articulos_candidatos = random.sample(articulos_candidatos, 5000)\n",
    "    print(f\"Paso 1: Se evaluarán {len(articulos_candidatos)} artículos candidatos.\")\n",
    "        \n",
    "    # 3. Crear el DataFrame para la predicción\n",
    "    df_predict = pd.DataFrame({'article_id': articulos_candidatos})\n",
    "    df_predict['customer_id'] = customer_id\n",
    "    \n",
    "    # 4. Enriquecer el DataFrame con las características del usuario y de los productos\n",
    "    df_predict = pd.merge(df_predict, user_profile, on='customer_id', how='left')\n",
    "    df_predict = pd.merge(df_predict, final_item_features, on='article_id', how='left')\n",
    "    \n",
    "    # 5. Preparar los datos para que coincidan con el formato de entrenamiento\n",
    "    print(\"Paso 2: Preparando características para el modelo...\")\n",
    "    # Seleccionar las mismas columnas de características que usamos para entrenar\n",
    "    X_pred = df_predict.drop(columns=['customer_id', 'article_id'])\n",
    "    \n",
    "    # Aplicar la misma codificación ordinal que en el entrenamiento\n",
    "    # Es crucial usar un encoder 'fittado' o re-codificar de la misma manera\n",
    "    categorical_features = X_pred.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    # Usamos el mismo OrdinalEncoder que antes, o creamos uno nuevo (para este ejemplo es suficiente)\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    # Importante: aquí hacemos fit_transform sobre las características de los candidatos\n",
    "    X_pred[categorical_features] = encoder.fit_transform(X_pred[categorical_features])\n",
    "\n",
    "    # 6. Alinear las columnas (Paso CRÍTICO para evitar errores)\n",
    "    # Nos aseguramos de que el DataFrame de predicción tenga exactamente las mismas columnas\n",
    "    # y en el mismo orden que los datos con los que se entrenó el modelo.\n",
    "    X_pred_aligned = X_pred.reindex(columns=training_columns, fill_value=0)\n",
    "\n",
    "    # 7. Realizar la predicción de probabilidades\n",
    "    print(\"Paso 3: Calculando puntuaciones de recomendación...\")\n",
    "    scores = modelo.predict_proba(X_pred_aligned)[:, 1]\n",
    "    \n",
    "    # 8. Unir puntuaciones y devolver el Top-N\n",
    "    df_predict['score'] = scores\n",
    "    recomendaciones = df_predict.sort_values('score', ascending=False).head(n_recommendations)\n",
    "    \n",
    "    # Añadir información legible (nombre del producto) para la salida final\n",
    "    recomendaciones_final = pd.merge(recomendaciones, df_articles[['article_id', 'prod_name']], \n",
    "                                     on='article_id', how='left')\n",
    "    \n",
    "    print(\"Paso 4: ¡Recomendaciones generadas!\")\n",
    "    return recomendaciones_final[['article_id', 'prod_name', 'score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EJEMPLO DE USO ---\n",
    "# Seleccionar un cliente aleatorio de nuestro diccionario de compras para asegurar que tiene historial\n",
    "lista_de_clientes = list(purchased_items_dict.keys())\n",
    "cliente_ejemplo = random.choice(lista_de_clientes)\n",
    "\n",
    "# Generar las recomendaciones para este cliente\n",
    "recomendaciones_para_cliente = generar_recomendaciones(cliente_ejemplo, modelo)\n",
    "\n",
    "# Mostrar los resultados de una manera bonita\n",
    "print(f\"\\n\\n✓✓✓ Top 10 Recomendaciones para el Cliente: {cliente_ejemplo} ✓✓✓\\n\")\n",
    "print(recomendaciones_para_cliente.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
